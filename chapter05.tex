The vague context-dependent nature of gradable adjective interpretation has been promisingly formalized in models within the Rational Speech Act framework -  a suite of game-theoretically oriented recursive models of pragmatic language understanding \parencite{goodman2016, lassiter2017adjectival, tessler2017warm}). However, up-to-date models either assumed an established felicitous comaprison class, or focused on simple utterances like "It's small". This chapter reviews the Rational Speech Act framework and prior models of gradable adjectives, to finally propose an extension to these existing models, incorporating the reference-predication trade-off hypothesis, allowing to flexibly incorporate reasoning about context and role of the noun. 
  
\section{Understanding Rational Speech Act Models}
threshold semantics, where the threshold is probabilistically inferred \parencite{lassiter2017adjectival} for a given comparison class.

S and L coordinate intended meaning; three levels. simple example: \parencite{goodman2016}

Understanding language requires more than just combining the meanings of single words in the sentences - it demands the listeners to make references to the context, their knowledge of the speaker, the world, and infinitely many other aspects relevant to our social environment the language is an indispensable part of. These socially relevant, pragmatic aspects of language going beyond the literal meaning of word and the sentence structure were first formalised by Grice (1975). Grice’s main idea is that speakers are generally cooperative when choosing their utterances in order to convey particular meanings. At the core of his proposal are four conversational maxims speakers are thought to strive to achieve: the maxims of relation, quantity, quality and manner. ….
Whereas Grice’s idea of rationality is the basis for the rational speech act framework, it has been shown that speakers might pursue additional other goals which need to be traded-off with these maximes. For example, politeness, overmodification etc...  
Grice’s ideas of language as rational action gave rise to the field of bayesian pragmatics, which integrates Gricean ideas with game-theoretic views on coordination, viewing language interpretation from the perspective of inference and decision making under uncertainty (Lassiter and Goodman, 2017).
This formalisation is a part of a growing body of Bayesian cognitive modeling tradition \parencite{tenenbaum2011grow}, wherein the agent’s beliefs about the world are represented as a probability distribution over propositions representing the possible worlds, and these beliefs are updated to a posterior distribution by the agent via Bayes’ rule upon learning about specific propositions. 

The Rational Speech Act framework is based on a form of Montague’s compositional semantics (1973). 
Previous Gradable Adjective Models
Lassiter and Goodman 2013

The most basic level of any RSA-model is L0, or the literal listener, who interprets an utterance according to its meaning by computing the probability of a state given the literal semantics of the utterance and the prior probability of the state.  The semantics of utterance are represented in truth-conditional Boolean semantics tradition, mapping utterances onto truth-values for specific states. Frank and Goodman (2012) define the informativeness of expressions in context as its surprisal, measuring how much it reduces surprisal about the referent. It can be derived that rational speakers choose words proportional to their specificity, as defined by the number of items the chosen word could apply to (Frank, Goodman 2012).   
“Speech acts are actions; thus, the speaker is modeled as a rational (Bayesian) actor. He chooses an action (e.g., an utterance) according to its utility. The speaker simulates taking an action, evaluates its utility, and chooses actions based on their utility. Rationality of choice is often defined as choice of an action that maximizes the agent’s (expected) utility. Here we consider a generalization in which speakers use a softmax function to approximate the (classical) rational choice to a variable degree” (problang)
The speaker chooses utterance u to communicate a state s to L0 , by trying to minimize effort for L0 to arrive from u at s, i.e., by minimizing surprisal of s given u, while trying to also keep utterance cost minimal. Having this utility function in mind, the S1 computes a probability distribution over utterances having an s in mind, in proportion to the speaker’s utility function Us (above), where alpha may control for speaker optimality  
“To interpret the utterance, the pragmatic listener considers the process that generated the utterance, in the first place” in the form of the S1. 

Lassiter \& Goodman (2013, 2017) first provided a model of gradable adjective interpretation within the RSA-framework, showing that a Bayesian approach can capture their vague meaning via inference over the latent threshold variable underlying the adjective semantics. Importantly, probabilistic reasoning provides tools to capture uncertainty over certain aspects of the message, in this particular case - the speaker’s intended meaning of the adjectival utterance.  
In this work, the relevant comparison class was assumed to be implicitly supplied. 

Tessler et al. 2017: 
Listeners use their world knowledge to infer the comparison class about what worlds are plausible given a specific comparison class, what comparison classes are likely to be talked about, and how a rational speaker would bhave in a given world and given a comparison class. 
→ To this end, the comparison class might still influence the referent prior for model 1.1. to the degree how likely the referent (i.e. it subordinate category) is to come from a given comparison class. Maybe one could formalize the prior over comparison classes (p(cc)) as some mixture of conceptual comparison classes with the context. This might shift the represented situation to more of a reasoning if the speaker was talking about a particular individual in the context, or some abstract individual from a certain comparison class. But for this model, it still boils down to just picking out a referent from different sets representing different classes (given our mainstream prior representation idea), irrespective of their properties (the original paper prior is not applicable to this model / question at hand, at all).

\section{Refpred-RSA}
\pt{tbd}